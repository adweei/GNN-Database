{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Graph Convolutional Network\n",
        "====================================\n",
        "\n",
        "**Author:** `Qi Huang <https://github.com/HQ01>`_, `Minjie Wang  <https://jermainewang.github.io/>`_,\n",
        "Yu Gai, Quan Gan, Zheng Zhang\n",
        "\n",
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The tutorial aims at gaining insights into the paper, with code as a mean\n",
        "    of explanation. The implementation thus is NOT optimized for running\n",
        "    efficiency. For recommended implementation, please refer to the `official\n",
        "    examples <https://github.com/dmlc/dgl/tree/master/examples>`_.</p></div>\n",
        "\n",
        "This is a gentle introduction of using DGL to implement Graph Convolutional\n",
        "Networks (Kipf & Welling et al., `Semi-Supervised Classification with Graph\n",
        "Convolutional Networks <https://arxiv.org/pdf/1609.02907.pdf>`_). We explain\n",
        "what is under the hood of the :class:`~dgl.nn.GraphConv` module.\n",
        "The reader is expected to learn how to define a new GNN layer using DGL's\n",
        "message passing APIs.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Overview\n",
        "------------------------------------------\n",
        "GCN from the perspective of message passing\n",
        "We describe a layer of graph convolutional neural network from a message\n",
        "passing perspective; the math can be found `here <math_>`_.\n",
        "It boils down to the following step, for each node $u$:\n",
        "\n",
        "1) Aggregate neighbors' representations $h_{v}$ to produce an\n",
        "intermediate representation $\\hat{h}_u$.  2) Transform the aggregated\n",
        "representation $\\hat{h}_{u}$ with a linear projection followed by a\n",
        "non-linearity: $h_{u} = f(W_{u} \\hat{h}_u)$.\n",
        "\n",
        "We will implement step 1 with DGL message passing, and step 2 by\n",
        "PyTorch ``nn.Module``.\n",
        "\n",
        "GCN implementation with DGL\n",
        "We first define the message and reduce function as usual.  Since the\n",
        "aggregation on a node $u$ only involves summing over the neighbors'\n",
        "representations $h_v$, we can simply use builtin functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import dgl.function as fn\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl import DGLGraph\n",
        "\n",
        "gcn_msg = fn.copy_u(u='h', out='m')\n",
        "gcn_reduce = fn.sum(msg='m', out='h')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then proceed to define the GCNLayer module. A GCNLayer essentially performs\n",
        "message passing on all the nodes then applies a fully-connected layer.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This is showing how to implement a GCN from scratch.  DGL provides a more\n",
        "   efficient :class:`builtin GCN layer module <dgl.nn.pytorch.conv.GraphConv>`.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        # Creating a local scope so that all the stored ndata and edata\n",
        "        # (such as the `'h'` ndata below) are automatically popped out\n",
        "        # when the scope exits.\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = feature\n",
        "            g.update_all(gcn_msg, gcn_reduce)\n",
        "            h = g.ndata['h']\n",
        "            return self.linear(h)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The forward function is essentially the same as any other commonly seen NNs\n",
        "model in PyTorch.  We can initialize GCN like any ``nn.Module``. For example,\n",
        "let's define a simple neural network consisting of two GCN layers. Suppose we\n",
        "are training the classifier for the cora dataset (the input feature size is\n",
        "1433 and the number of classes is 7). The last GCN layer computes node embeddings,\n",
        "so the last layer in general does not apply activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (layer1): GCNLayer(\n",
            "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
            "  )\n",
            "  (layer2): GCNLayer(\n",
            "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = GCNLayer(1433, 16)\n",
        "        self.layer2 = GCNLayer(16, 7)\n",
        "    \n",
        "    def forward(self, g, features):\n",
        "        x = F.relu(self.layer1(g, features))\n",
        "        x = self.layer2(g, x)\n",
        "        return x\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the cora dataset using DGL's built-in data module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.data import CoraGraphDataset\n",
        "def load_cora_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    g = dataset[0]\n",
        "    features = g.ndata['feat']\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    print(\"train mask: \",th.bincount(train_mask.type(th.int)))\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    print(\"test mask: \",th.bincount(test_mask.type(th.int)))\n",
        "    return g, features, labels, train_mask, test_mask"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When a model is trained, we can use the following method to evaluate\n",
        "the performance of the model on the test dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(model, g, features, labels, mask):\n",
        "    model.eval()\n",
        "    with th.no_grad():\n",
        "        logits = model(g, features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        _, indices = th.max(logits, dim=1)\n",
        "        correct = th.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then train the network as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "train mask:  tensor([2568,  140])\n",
            "test mask:  tensor([1708, 1000])\n",
            "Epoch 00000 | Loss 1.9525 | Test Acc 0.2130 | Time(s) nan\n",
            "Epoch 00001 | Loss 1.8263 | Test Acc 0.4590 | Time(s) nan\n",
            "Epoch 00002 | Loss 1.6796 | Test Acc 0.6210 | Time(s) nan\n",
            "Epoch 00003 | Loss 1.5292 | Test Acc 0.6060 | Time(s) 0.0130\n",
            "Epoch 00004 | Loss 1.4033 | Test Acc 0.5990 | Time(s) 0.0120\n",
            "Epoch 00005 | Loss 1.2903 | Test Acc 0.6370 | Time(s) 0.0113\n",
            "Epoch 00006 | Loss 1.1806 | Test Acc 0.6920 | Time(s) 0.0115\n",
            "Epoch 00007 | Loss 1.0765 | Test Acc 0.7250 | Time(s) 0.0110\n",
            "Epoch 00008 | Loss 0.9806 | Test Acc 0.7340 | Time(s) 0.0106\n",
            "Epoch 00009 | Loss 0.8923 | Test Acc 0.7370 | Time(s) 0.0104\n",
            "Epoch 00010 | Loss 0.8105 | Test Acc 0.7370 | Time(s) 0.0103\n",
            "Epoch 00011 | Loss 0.7352 | Test Acc 0.7290 | Time(s) 0.0102\n",
            "Epoch 00012 | Loss 0.6676 | Test Acc 0.7250 | Time(s) 0.0102\n",
            "Epoch 00013 | Loss 0.6080 | Test Acc 0.7260 | Time(s) 0.0101\n",
            "Epoch 00014 | Loss 0.5554 | Test Acc 0.7280 | Time(s) 0.0099\n",
            "Epoch 00015 | Loss 0.5048 | Test Acc 0.7350 | Time(s) 0.0099\n",
            "Epoch 00016 | Loss 0.4575 | Test Acc 0.7390 | Time(s) 0.0098\n",
            "Epoch 00017 | Loss 0.4147 | Test Acc 0.7470 | Time(s) 0.0097\n",
            "Epoch 00018 | Loss 0.3755 | Test Acc 0.7510 | Time(s) 0.0098\n",
            "Epoch 00019 | Loss 0.3398 | Test Acc 0.7450 | Time(s) 0.0098\n",
            "Epoch 00020 | Loss 0.3076 | Test Acc 0.7510 | Time(s) 0.0097\n",
            "Epoch 00021 | Loss 0.2791 | Test Acc 0.7570 | Time(s) 0.0096\n",
            "Epoch 00022 | Loss 0.2537 | Test Acc 0.7570 | Time(s) 0.0096\n",
            "Epoch 00023 | Loss 0.2310 | Test Acc 0.7650 | Time(s) 0.0096\n",
            "Epoch 00024 | Loss 0.2105 | Test Acc 0.7660 | Time(s) 0.0095\n",
            "Epoch 00025 | Loss 0.1919 | Test Acc 0.7640 | Time(s) 0.0095\n",
            "Epoch 00026 | Loss 0.1747 | Test Acc 0.7680 | Time(s) 0.0094\n",
            "Epoch 00027 | Loss 0.1590 | Test Acc 0.7700 | Time(s) 0.0094\n",
            "Epoch 00028 | Loss 0.1445 | Test Acc 0.7700 | Time(s) 0.0095\n",
            "Epoch 00029 | Loss 0.1313 | Test Acc 0.7680 | Time(s) 0.0094\n",
            "Epoch 00030 | Loss 0.1192 | Test Acc 0.7650 | Time(s) 0.0094\n",
            "Epoch 00031 | Loss 0.1083 | Test Acc 0.7620 | Time(s) 0.0095\n",
            "Epoch 00032 | Loss 0.0985 | Test Acc 0.7580 | Time(s) 0.0095\n",
            "Epoch 00033 | Loss 0.0896 | Test Acc 0.7540 | Time(s) 0.0094\n",
            "Epoch 00034 | Loss 0.0816 | Test Acc 0.7530 | Time(s) 0.0094\n",
            "Epoch 00035 | Loss 0.0744 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00036 | Loss 0.0680 | Test Acc 0.7470 | Time(s) 0.0094\n",
            "Epoch 00037 | Loss 0.0621 | Test Acc 0.7460 | Time(s) 0.0094\n",
            "Epoch 00038 | Loss 0.0568 | Test Acc 0.7470 | Time(s) 0.0094\n",
            "Epoch 00039 | Loss 0.0521 | Test Acc 0.7480 | Time(s) 0.0094\n",
            "Epoch 00040 | Loss 0.0478 | Test Acc 0.7480 | Time(s) 0.0093\n",
            "Epoch 00041 | Loss 0.0440 | Test Acc 0.7490 | Time(s) 0.0093\n",
            "Epoch 00042 | Loss 0.0405 | Test Acc 0.7510 | Time(s) 0.0093\n",
            "Epoch 00043 | Loss 0.0374 | Test Acc 0.7520 | Time(s) 0.0093\n",
            "Epoch 00044 | Loss 0.0346 | Test Acc 0.7530 | Time(s) 0.0093\n",
            "Epoch 00045 | Loss 0.0320 | Test Acc 0.7510 | Time(s) 0.0093\n",
            "Epoch 00046 | Loss 0.0298 | Test Acc 0.7530 | Time(s) 0.0093\n",
            "Epoch 00047 | Loss 0.0277 | Test Acc 0.7520 | Time(s) 0.0093\n",
            "Epoch 00048 | Loss 0.0258 | Test Acc 0.7490 | Time(s) 0.0093\n",
            "Epoch 00049 | Loss 0.0241 | Test Acc 0.7490 | Time(s) 0.0093\n",
            "Epoch 00050 | Loss 0.0225 | Test Acc 0.7490 | Time(s) 0.0093\n",
            "Epoch 00051 | Loss 0.0211 | Test Acc 0.7490 | Time(s) 0.0093\n",
            "Epoch 00052 | Loss 0.0198 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00053 | Loss 0.0186 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00054 | Loss 0.0175 | Test Acc 0.7490 | Time(s) 0.0096\n",
            "Epoch 00055 | Loss 0.0165 | Test Acc 0.7490 | Time(s) 0.0096\n",
            "Epoch 00056 | Loss 0.0156 | Test Acc 0.7480 | Time(s) 0.0096\n",
            "Epoch 00057 | Loss 0.0148 | Test Acc 0.7480 | Time(s) 0.0096\n",
            "Epoch 00058 | Loss 0.0140 | Test Acc 0.7480 | Time(s) 0.0096\n",
            "Epoch 00059 | Loss 0.0133 | Test Acc 0.7480 | Time(s) 0.0096\n",
            "Epoch 00060 | Loss 0.0127 | Test Acc 0.7480 | Time(s) 0.0096\n",
            "Epoch 00061 | Loss 0.0121 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00062 | Loss 0.0115 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00063 | Loss 0.0110 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00064 | Loss 0.0106 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00065 | Loss 0.0101 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00066 | Loss 0.0097 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00067 | Loss 0.0093 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00068 | Loss 0.0090 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00069 | Loss 0.0086 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00070 | Loss 0.0083 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00071 | Loss 0.0080 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00072 | Loss 0.0078 | Test Acc 0.7470 | Time(s) 0.0095\n",
            "Epoch 00073 | Loss 0.0075 | Test Acc 0.7480 | Time(s) 0.0095\n",
            "Epoch 00074 | Loss 0.0073 | Test Acc 0.7490 | Time(s) 0.0095\n",
            "Epoch 00075 | Loss 0.0071 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00076 | Loss 0.0068 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00077 | Loss 0.0066 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00078 | Loss 0.0065 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00079 | Loss 0.0063 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00080 | Loss 0.0061 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00081 | Loss 0.0059 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00082 | Loss 0.0058 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00083 | Loss 0.0056 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00084 | Loss 0.0055 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00085 | Loss 0.0054 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00086 | Loss 0.0052 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00087 | Loss 0.0051 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00088 | Loss 0.0050 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00089 | Loss 0.0049 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00090 | Loss 0.0048 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00091 | Loss 0.0047 | Test Acc 0.7500 | Time(s) 0.0095\n",
            "Epoch 00092 | Loss 0.0046 | Test Acc 0.7500 | Time(s) 0.0095\n",
            "Epoch 00093 | Loss 0.0045 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00094 | Loss 0.0044 | Test Acc 0.7500 | Time(s) 0.0095\n",
            "Epoch 00095 | Loss 0.0043 | Test Acc 0.7500 | Time(s) 0.0094\n",
            "Epoch 00096 | Loss 0.0042 | Test Acc 0.7500 | Time(s) 0.0095\n",
            "Epoch 00097 | Loss 0.0042 | Test Acc 0.7510 | Time(s) 0.0095\n",
            "Epoch 00098 | Loss 0.0041 | Test Acc 0.7510 | Time(s) 0.0094\n",
            "Epoch 00099 | Loss 0.0040 | Test Acc 0.7510 | Time(s) 0.0094\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "g, features, labels, train_mask, test_mask = load_cora_data()\n",
        "# Add edges between each node and itself to preserve old node representations\n",
        "g.add_edges(g.nodes(), g.nodes())\n",
        "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
        "dur = []\n",
        "for epoch in range(100):\n",
        "    if epoch >=3:\n",
        "        t0 = time.time()\n",
        "\n",
        "    net.train()\n",
        "    logits = net(g, features)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch >=3:\n",
        "        dur.append(time.time() - t0)\n",
        "    \n",
        "    acc = evaluate(net, g, features, labels, test_mask)\n",
        "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
        "            epoch, loss.item(), acc, np.mean(dur)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "GCN in one formula\n",
        "------------------\n",
        "Mathematically, the GCN model follows this formula:\n",
        "\n",
        "$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$\n",
        "\n",
        "Here, $H^{(l)}$ denotes the $l^{th}$ layer in the network,\n",
        "$\\sigma$ is the non-linearity, and $W$ is the weight matrix for\n",
        "this layer. $\\tilde{D}$ and $\\tilde{A}$ are separately the degree\n",
        "and adjacency matrices for the graph. With the superscript ~, we are referring\n",
        "to the variant where we add additional edges between each node and itself to\n",
        "preserve its old representation in graph convolutions. The shape of the input\n",
        "$H^{(0)}$ is $N \\times D$, where $N$ is the number of nodes\n",
        "and $D$ is the number of input features. We can chain up multiple\n",
        "layers as such to produce a node-level representation output with shape\n",
        "$N \\times F$, where $F$ is the dimension of the output node\n",
        "feature vector.\n",
        "\n",
        "The equation can be efficiently implemented using sparse matrix\n",
        "multiplication kernels (such as Kipf's\n",
        "`pygcn <https://github.com/tkipf/pygcn>`_ code). The above DGL implementation\n",
        "in fact has already used this trick due to the use of builtin functions.\n",
        "\n",
        "Note that the tutorial code implements a simplified version of GCN where we\n",
        "replace $\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$ with\n",
        "$\\tilde{A}$. For a full implementation, see our example\n",
        "`here  <https://github.com/dmlc/dgl/tree/master/examples/pytorch/gcn>`_.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.15 ('dgl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd41fe153378347448e1c201e7672b8e1a74a52b34c8d7e7e6593400a70fc6a0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
