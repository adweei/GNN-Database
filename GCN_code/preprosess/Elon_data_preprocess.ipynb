{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import dgl\n",
    "import re\n",
    "import torch as th\n",
    "from dgl.data.utils import load_graphs\n",
    "from dgl.data.utils import save_graphs\n",
    "# import torch.nn.functional as F \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elon_mask_base_graph_data_dir = 'D:/GCN_Twitter/ElonMusk/2023-02-16/'          #D:/GNN/new_data/MyResearch-main/MyResearch-main/ElonMusk/2023-05-01/\n",
    "elon_mask_test_graph_data_dir = 'D:/GCN_Twitter/ElonMusk/2023-02-20/'          #D:/GNN/new_data/MyResearch-main/MyResearch-main/ElonMusk/2023-05-06/\n",
    "\n",
    "hashtag_dir = elon_mask_base_graph_data_dir + 'hashtags/'\n",
    "user_profile_dir = elon_mask_base_graph_data_dir + 'user_profile/'\n",
    "user_activelabel_dir = elon_mask_base_graph_data_dir + 'elon_mask_base_graph_data_dirlabel/'\n",
    "\n",
    "base_graph_dir = elon_mask_base_graph_data_dir + 'base_graph_for_model/'\n",
    "gat_base_graph_dir = base_graph_dir + 'GAT_graph_model/'\n",
    "gat_no_edge_weight_base_graph_dir = base_graph_dir + 'GAT_graph_no_weight_model2/'\n",
    "\n",
    "predict_graph_dir = elon_mask_test_graph_data_dir + 'data_graph/'\n",
    "follow_relationship = elon_mask_base_graph_data_dir + 'followers/'\n",
    "tweet_retweet_relation = elon_mask_base_graph_data_dir + 'retweeters/'\n",
    "test_tweet_label =  elon_mask_test_graph_data_dir + 'label/'\n",
    "base_tweet_retweet_distribution = elon_mask_base_graph_data_dir + 'retweet_distribution/'\n",
    "test_tweet_retweet_distribution = elon_mask_test_graph_data_dir + 'retweet_distribution/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA01(data, k=2):\n",
    "    X = torch.from_numpy(data)\n",
    "    X_mean = torch.mean(X, 0)\n",
    "    X = X - X_mean.expand_as(X)\n",
    "    # SVD\n",
    "    U,S,V = torch.svd(torch.t(X))\n",
    "    return torch.mm(X,U[:,:k])\n",
    "\n",
    "def np_norm(data):\n",
    "    return (data-np.min(data))/(np.max(data)-np.min(data))\n",
    "\n",
    "def self_def_norm(data,floor,ceiling):\n",
    "    return ((data-np.min(data))/(np.max(data)-np.min(data))) * (ceiling - floor) - (0 - floor)\n",
    "\n",
    "# def determine_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')\n",
    "#user流水號讀入\n",
    "data = json.load(f)                                                            \n",
    "print(data.values())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = pd.read_excel(elon_mask_base_graph_data_dir + 'ElonMusk.xlsx')\n",
    "\n",
    "#處理tweet_id轉成流水號\n",
    "tweet_id = dict()                                                               \n",
    "for row in range(0,len(workbook),1):\n",
    "    tweet_id[int(workbook.id[row])] = (row + 1)\n",
    "\n",
    "with open(elon_mask_base_graph_data_dir + \"tweet_id.json\", \"w\") as outfile:\n",
    "    json.dump(tweet_id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening JSON file\n",
    "f = open(hashtag_dir + 'hashtag_count.json')                                   \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "hash_count_data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "selected_hashtag = list()\n",
    "# 處理被提及次數超過20次的hashtag\n",
    "for item in hash_count_data.items():\n",
    "    if item[1] >= 20:\n",
    "        selected_hashtag.append(item[0])\n",
    "# Closing file\n",
    "f.close()\n",
    "with open(hashtag_dir + \"selected_hashtag.json\", \"w\") as outfile:\n",
    "    json.dump(selected_hashtag, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(hashtag_dir + 'hashtag_user_mention.json')\n",
    "hashtag_data = json.load(f)\n",
    "f.close()                                                                             #這邊處理哪個hashtag被哪個user提到\n",
    "f2 = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')\n",
    "user_id = json.load(f2) \n",
    "f2.close()\n",
    "f3 = open(hashtag_dir + 'selected_hashtag.json')\n",
    "selected_hashtag_data = json.load(f3)\n",
    "f3.close()\n",
    "\n",
    "hashtag_map = np.zeros((len(user_id), len(selected_hashtag_data)))                     #先建立hashTag等大的陣列\n",
    "key_list = list(hashtag_data.keys())\n",
    "\n",
    "count = 0\n",
    "for keys in hashtag_data:                                                              #看現在被選到的hashtag是否有在我們所選擇的hashtag範圍內\n",
    "    if keys in selected_hashtag_data:                                                  #如果是我們需要的hashtag，那就把有提到它的人都列出來\n",
    "        for user in hashtag_data[keys]:                                                #根據前面的結果，把陣列內對應位置的數值改成1，代表該人有提到該hashtag\n",
    "            try:\n",
    "                hashtag_map[user_id[user]][key_list.index(keys)] = 1\n",
    "            except:\n",
    "                count += 1\n",
    "np.save(hashtag_dir + \"orgin_hashtag_map.json\", hashtag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hashtag_map = np.load(hashtag_dir + \"orgin_hashtag_map.json.npy\")\n",
    "print(hashtag_map.shape)\n",
    "#做PCA降維到200\n",
    "user_feature = PCA01(hashtag_map,200) \n",
    "user_feature = user_feature.numpy()\n",
    " \n",
    "#使用sklearn PCA降維到200\n",
    "# pca = PCA(n_components = 200)\n",
    "# pca.fit(hashtag_map)                            # 計算特徵\n",
    "# user_feature_sk = pca.transform(hashtag_map)\n",
    "print(user_feature.shape)\n",
    "\n",
    "np.save(elon_mask_base_graph_data_dir + 'user_feature', user_feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畫PCA降維分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# pic = sns.kdeplot(user_feature_sk[:,1:3], clip = (-3,3))\n",
    "for i in range(50):\n",
    "    pic = sns.kdeplot(user_feature[:,i*4:(i*4+4)], clip = (-3,3))\n",
    "    plt.savefig(\"D:/GCN_Twitter/ElonMusk/2023-02-16/PCA_picture/picture_01\" + str(i * 4) + \"~\" + str(i * 4 +4))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_feature_sk_inverse_transform = pca.inverse_transform(user_feature_sk)              # 逆轉換\n",
    "# # test = pca.explained_variance_\n",
    "# # print(test)\n",
    "# # plt.scatter(hashtag_map[:, 0:5], hashtag_map[:, 5:10], alpha=0.4, edgecolor =\"green\")\n",
    "# plt.scatter(user_feature_sk_inverse_transform[:, 0:5], user_feature_sk_inverse_transform[:, 5:10], alpha=0.4,\n",
    "#             edgecolor =\"red\")\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA降維後，加入自訂義4個feature\n",
    "\n",
    "#讀id user\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')\n",
    "user_id = json.load(id)                                                                          #\n",
    "id.close()\n",
    "\n",
    "# 將Hashtag的pca結果(200個)和我們自訂義的4個feature做連接並且作輸出\n",
    "all_user_profile = pd.read_excel(elon_mask_base_graph_data_dir + 'user_profile/Profile.xlsx')\n",
    "user_feature = np.load(elon_mask_base_graph_data_dir + 'user_feature.npy')\n",
    "# user_feature = np.transpose(np_norm(np.transpose(user_feature)))\n",
    "\n",
    "selfdefine_feature = ['followers_count', 'following_count', 'tweet_count', 'verified']\n",
    "allperson_features = list()\n",
    "allperson_popular = [0.1] * len(all_user_profile.index) #建立all_user_profile.index的個數的[0.1]\n",
    "for keys in user_id:\n",
    "    profile_feature = all_user_profile.loc[all_user_profile['id'].astype(str) == keys, \n",
    "                                           ['followers_count', 'following_count', 'tweet_count', 'verified']].to_numpy(dtype=int)\n",
    "    user_index = all_user_profile.index[all_user_profile['id'].astype(str) == keys].tolist()\n",
    "    allperson_features.append(profile_feature[0])\n",
    "    if(profile_feature[0][0] != 0):                                                     #log沒有等於0\n",
    "        if(profile_feature[0][0] != 1):                                                 #log1 = 0,加權不能為0\n",
    "            # if(math.log(profile_feature[0][0],10) !=1):\n",
    "            allperson_popular[user_index[0]] = math.log(profile_feature[0][0],10)       #popular做log壓縮\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_profile['popular'] = allperson_popular\n",
    "all_user_profile['id'] = all_user_profile['id'].astype(str)\n",
    "allperson_features = np.transpose(allperson_features).astype(float)\n",
    "# 調整normalize的範圍\n",
    "allperson_features[0] = self_def_norm(allperson_features[0], -1, 1)\n",
    "allperson_features[1] = self_def_norm(allperson_features[1], -1, 1)\n",
    "allperson_features[2] = self_def_norm(allperson_features[2], -1, 1)\n",
    "\n",
    "allperson_features = np.transpose(allperson_features)\n",
    "user_feature = np.concatenate((user_feature, allperson_features), axis=1)\n",
    "np.save(elon_mask_base_graph_data_dir + 'user_feature', user_feature)\n",
    "all_user_profile.to_excel(elon_mask_base_graph_data_dir + 'user_profile/Profile_new.xlsx',index=False)\n",
    "\n",
    "print(user_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#要篩選的tweet id給讀入\n",
    "target_tweet = pd.read_excel(elon_mask_base_graph_data_dir + 'compare5.xlsx')   \n",
    "#retweet >= 1500 的推文數 0~26是27篇 \n",
    "thershold = 26                                                                                              \n",
    "target_tweet = target_tweet.drop(index=list(range(thershold + 1,len(target_tweet))))\n",
    "target_tweet['id'] = target_tweet['id'].astype(str)\n",
    "#讀user id\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')       \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "#取兩個set之間的交集，藉以找出重合的target user(哪些人有追蹤這篇推文)\n",
    "user_id_list = list(user_id.keys())                                                                          \n",
    "user_id_list = [int(x) for x in user_id_list]\n",
    "user_id_set = set(user_id_list)\n",
    "\n",
    "# folder path                                                                              #將各推文轉推情形的檔名紀錄\n",
    "tweet_retweet_path = elon_mask_base_graph_data_dir + 'retweeters/'\n",
    "# list to store files\n",
    "all_tweet_retweet_file = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(tweet_retweet_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(tweet_retweet_path, path)):\n",
    "        all_tweet_retweet_file.append(path)\n",
    "\n",
    "all_label = list()\n",
    "label_order = list()\n",
    "#在所有的json檔中做尋找\n",
    "for file in all_tweet_retweet_file:                                                                     \n",
    "    retweet_file = open(tweet_retweet_path + file)\n",
    "    part_tweet_retweet_file = json.load(retweet_file)                                                   \n",
    "    for per_tweet in part_tweet_retweet_file.keys():\n",
    "        #檢查當前tweet是否在target tweet名單內，有的話紀錄它在原本檔案中的index和有哪些人轉推\n",
    "        inner = target_tweet['id'].isin([per_tweet])                                                    \n",
    "        if (inner.sum() != 0):                                                                          \n",
    "            label_order.append(target_tweet[inner].index.tolist())\n",
    "            # target user和此tweet中的retweeter的交集\n",
    "            target_tweet_retweet = user_id_set.intersection(set(part_tweet_retweet_file[per_tweet]))\n",
    "            # print(len(target_tweet_retweet))\n",
    "            all_label.append(target_tweet_retweet)\n",
    "            print('order: ',all_label)\n",
    "            # print('retweet count: ',len(target_tweet_retweet))\n",
    "\n",
    "for number in range(0, len(all_label), 1):\n",
    "    per_tweet_label = np.zeros(len(user_id))\n",
    "    for user in all_label[number]:\n",
    "        per_tweet_label[user_id[str(user)]] = 1\n",
    "    np.save(elon_mask_base_graph_data_dir + 'label/label_for_' + str(label_order[number][0]), per_tweet_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "doc_num = len(os.listdir('D:/GCN_Twitter/ElonMusk/2023-02-16/label/'))\n",
    "for i in range(doc_num):\n",
    "    count = 0\n",
    "    label_l = np.load(elon_mask_base_graph_data_dir + 'label/label_for_' + str(i) + '.npy')\n",
    "#     print(label_l.shape)\n",
    "    for j in range(len(label_l)):\n",
    "        if(label_l[j] == 1):\n",
    "            count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "doc_num = len(os.listdir('D:/GCN_Twitter/ElonMusk/2023-02-16/label/'))\n",
    "for i in range(doc_num):\n",
    "    count = 0\n",
    "    label_l = np.load(elon_mask_base_graph_data_dir + 'label/label_for_' + str(i) + '.npy')\n",
    "#     print(label_l.shape)\n",
    "    \n",
    "    print(np.sum(label_l))\n",
    "    # for j in range(len(label_l)):\n",
    "    #     if(label_l[j] == 1):\n",
    "    #         count += 1\n",
    "    # print(count)\n",
    "# label_l = np.load(elon_mask_base_graph_data_dir + 'label/label_for_' + str(i) + '.npy')\n",
    "# for i in range(len(label_l)):\n",
    "#     if(label_l[i] == 1):\n",
    "#         count += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自動建立資料夾在base_graph_for_model內，數量為label內檔案的個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "doc_num = len(os.listdir('D:/GCN_Twitter/ElonMusk/2023-02-16/label/'))\n",
    "for i in range(doc_num):\n",
    "    # if not os.path.isdir(elon_mask_base_graph_data_dir + 'base_graph_for_model/' + str(i)):\n",
    "    #     os.mkdir('D:/GCN_Twitter/ElonMusk/2023-02-16/base_graph_for_model/' + str(i))\n",
    "\n",
    "    # if not os.path.isdir(elon_mask_base_graph_data_dir + 'base_graph_for_model/' + 'GAT_graph_model/' + str(i)):\n",
    "    #     os.mkdir('D:/GCN_Twitter/ElonMusk/2023-02-16/base_graph_for_model/GAT_graph_model/' + str(i))  \n",
    "           \n",
    "    # if not os.path.isdir(elon_mask_base_graph_data_dir + 'base_graph_for_model/' + 'GAT_graph_no_weight_model2/' + str(i)):\n",
    "    #     os.mkdir('D:/GCN_Twitter/ElonMusk/2023-02-16/base_graph_for_model/GAT_graph_no_weight_model2/' + str(i))  \n",
    "\n",
    "    if not os.path.isdir(elon_mask_base_graph_data_dir + 'base_graph_for_model/' + 'GAT_FocalLoss/' + str(i)):\n",
    "        os.mkdir('D:/GCN_Twitter/ElonMusk/2023-02-16/base_graph_for_model/GAT_FocalLoss/' + str(i))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#這邊是建立base graph(edge關聯性)\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')                                       #把userid給讀入\n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "src_vertx = dict()\n",
    "\n",
    "def find_in_target(target_dict,check):\n",
    "    if(str(check) in target_dict):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "all_user_follow_file = []                                                                                 #將所有follower檔案的檔名紀錄\n",
    "for path in os.listdir(follow_relationship):\n",
    "    if os.path.isfile(os.path.join(follow_relationship, path)):\n",
    "        if(path != 'track.py'):\n",
    "            all_user_follow_file.append(path)\n",
    "\n",
    "all_user_retweet_file = []                                                                                 #將所有retweet紀錄檔案的檔名紀錄\n",
    "for path in os.listdir(tweet_retweet_relation):\n",
    "    if os.path.isfile(os.path.join(tweet_retweet_relation, path)):\n",
    "        all_user_retweet_file.append(path)\n",
    "\n",
    "for follow_file in all_user_follow_file:\n",
    "    follow = open(follow_relationship + follow_file)                                                   #把user 的follow情況給讀入\n",
    "    user_follower = json.load(follow)\n",
    "    follow.close()\n",
    "\n",
    "    for user in user_follower:                                                                         #把user情況轉流水號先記錄    格式是: src:\n",
    "        if (user in user_id):                                                                          #                                  dst:\n",
    "            dst_vertx = dict()                                                                         #                                  次數\n",
    "            for followers in user_follower[user]:\n",
    "                if(str(followers) in user_id):\n",
    "                    dst_vertx[user_id[str(followers)]] = 1\n",
    "            src_vertx[user_id[user]] = dst_vertx\n",
    "\n",
    "\n",
    "count = 0\n",
    "for retweet_file in all_user_retweet_file:\n",
    "    retweet = open(tweet_retweet_relation + retweet_file)                                                   #把user 的follow情況給讀入\n",
    "    user_tweet_retweet = json.load(retweet)\n",
    "    retweet.close()\n",
    "    for tweet in user_tweet_retweet:                                                                        #根據每篇推文\n",
    "        for ever_user in user_tweet_retweet[tweet]:                                                         #檢查底下所有的user\n",
    "            if(find_in_target(user_id,ever_user)):                                                          #檢查是否是最後的target user\n",
    "                if(user_id[str(ever_user)] in src_vertx):                                                   #檢查是否在src_dst的src列表裡\n",
    "                    leaf_user = list()\n",
    "                    leaf_user = user_tweet_retweet[tweet].copy()\n",
    "                    leaf_user.remove(ever_user)                                                             #需要檢查除了當前的user以外 \n",
    "                                                                                                            #也有轉推同一篇的user是否有出現在src_dst的dst列表裡\n",
    "                    for weight_change_user in leaf_user:\n",
    "                        if(find_in_target(user_id,weight_change_user)):                                     #檢查是否是最後的target user\n",
    "                            if(user_id[str(weight_change_user)] in src_vertx[user_id[str(ever_user)]]):     #檢查是否出現在src_dst的dst列表\n",
    "                                # print('src:',user_id[str(ever_user)])\n",
    "                                # print('dst:',user_id[str(weight_change_user)])\n",
    "                                # print('dict:',src_vertx[user_id[str(ever_user)]])\n",
    "                                count += 1\n",
    "                                src_vertx[user_id[str(ever_user)]][user_id[str(weight_change_user)]] += 1    #若有出現，則將次數+1\n",
    "\n",
    "\n",
    "with open(elon_mask_base_graph_data_dir + \"user_relation.json\", \"w\") as outfile:\n",
    "    json.dump(src_vertx, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找weight最大的\n",
    "a = open(elon_mask_base_graph_data_dir + 'user_relation.json')                                       \n",
    "a_id = json.load(a)\n",
    "a.close()\n",
    "\n",
    "max_src_n = 0\n",
    "max_dst_n = 0\n",
    "max_weight = 0\n",
    "for src_n in a_id:\n",
    "    for dst_n in src_n:\n",
    "        for weight in dst_n:\n",
    "            if(int(weight) > max_weight):\n",
    "                max_src_n = src_n\n",
    "                max_dst_n = dst_n\n",
    "                max_weight = int(weight)\n",
    "print('max source node is :', max_src_n)\n",
    "print('max destion node is :', max_dst_n)\n",
    "print('max node weight is :', max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user id給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')                                    \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "vertx_number = len(user_id)\n",
    "del user_id\n",
    "\n",
    "src_vertx = dict()\n",
    "#把所有edge資訊讀入給讀入\n",
    "edge = open(elon_mask_base_graph_data_dir + 'user_relation.json')                                       \n",
    "all_edge_collection = json.load(edge)\n",
    "edge.close()\n",
    "\n",
    "src_node = np.array([])\n",
    "dst_node = np.array([])\n",
    "weight = np.array([])\n",
    "#根據前面處裡完的src, dst, weight的dict解析結果\n",
    "for src_vertx in all_edge_collection:                                                                   \n",
    "    for dst_vertx in all_edge_collection[src_vertx]:\n",
    "        # print('src node is: ',src_vertx)\n",
    "        # print('dst node is: ',dst_vertx)\n",
    "        # print('edge weight is: ',all_edge_collection[src_vertx][dst_vertx])\n",
    "        # print('--------------------------------------')\n",
    "        #將個別的結果陣列儲存\n",
    "        src_node = np.append(src_node,src_vertx)\n",
    "        dst_node = np.append(dst_node,dst_vertx)\n",
    "        weight = np.append(weight,int(all_edge_collection[src_vertx][dst_vertx]))                       \n",
    "del all_edge_collection\n",
    "\n",
    "weight = np.reshape(weight,(weight.shape[0],1))\n",
    "np.save(elon_mask_base_graph_data_dir + 'Graph/source_vertx_collection', src_node)\n",
    "np.save(elon_mask_base_graph_data_dir + 'Graph/destion_vertx_collection', dst_node)\n",
    "np.save(elon_mask_base_graph_data_dir + 'Graph/edge_weight_collection', weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73469\n"
     ]
    }
   ],
   "source": [
    "#建2/11~15有weight的有向圖\n",
    "\n",
    "#把user id給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')                                       \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "vertx_number = len(user_id)\n",
    "del user_id\n",
    "\n",
    "src_node = np.load(elon_mask_base_graph_data_dir + 'Graph/source_vertx_collection.npy')\n",
    "dst_node = np.load(elon_mask_base_graph_data_dir + 'Graph/destion_vertx_collection.npy')\n",
    "weight = np.load(elon_mask_base_graph_data_dir + 'Graph/edge_weight_collection.npy')\n",
    "model_graph_dir = elon_mask_base_graph_data_dir + 'user_feature.npy'\n",
    "vertx_feature = np.load(model_graph_dir)\n",
    "#將前面得到的src和dst的矩陣當作資料，一個個加入到圖中\n",
    "base_graph = dgl.graph(([],[]),num_nodes = vertx_number)                                                  \n",
    "base_graph.add_edges(src_node.astype(int),dst_node.astype(int))\n",
    "graph_base_weight = th.from_numpy(weight)\n",
    "graph_base_weight = graph_base_weight.to(th.int64)\n",
    "graph_feature = th.from_numpy(vertx_feature).to(th.float32)\n",
    "base_graph.edata['weight'] = graph_base_weight * 0.1\n",
    "base_graph.ndata['feature'] = graph_feature\n",
    "# print(base_graph.edata['weight'])\n",
    "#將graph的結果輸出\n",
    "# save_graphs(elon_mask_base_graph_data_dir + \"Graph/base_graph.bin\", [base_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2/11~15 weighted module graph\n",
    "\n",
    "all_base_graph_dir = list()\n",
    "#把userid給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')           \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "\n",
    "#需要建立幾個model\n",
    "for it in os.scandir(elon_mask_base_graph_data_dir + 'base_graph_for_model/'):    \n",
    "    if it.is_dir():\n",
    "        all_base_graph_dir.append(it.path + \"/\")\n",
    "\n",
    "popular = pd.read_excel(user_profile_dir + 'Profile_new.xlsx')\n",
    "popular[['id','popular']] = popular[['id','popular']].astype(str)\n",
    "popular = popular[['id','popular']]\n",
    "\n",
    "#將建好的graph讀取出來之後把label放上去 並且對應edge去檢查是否active，再去更改edge_weight\n",
    "dataset,labels =  load_graphs(elon_mask_base_graph_data_dir + \"Graph/base_graph.bin\")     \n",
    "base_graph = dataset[0]\n",
    "u,v = base_graph.edges()\n",
    "src_vertx = np.array(u)\n",
    "edge_weight = base_graph.edata['weight'].numpy()\n",
    "# print(np.where(edge_weight == 0))\n",
    "\n",
    "for per_tweet in range(len(all_base_graph_dir)):#\n",
    "    Save_train_Path = all_base_graph_dir[per_tweet] + 'train/'\n",
    "    Save_test_Path = all_base_graph_dir[per_tweet] + 'test/'\n",
    "    tweet_number = all_base_graph_dir[per_tweet].split(\"/\")\n",
    "    tweet_number = tweet_number[-2]\n",
    "\n",
    "    label = np.load(elon_mask_base_graph_data_dir + 'label/label_for_' + str(tweet_number) + '.npy')\n",
    "    base_graph.ndata['label'] = th.from_numpy(label).type(th.LongTensor)\n",
    "    isolate_count = 0\n",
    "    for vertx in user_id.keys():\n",
    "        try:\n",
    "            would_be_change_edge_list = np.where(src_vertx == user_id[vertx])\n",
    "            # print('orgin:',edge_weight[would_be_change_edge_list])\n",
    "            if(label[user_id[vertx]] == 1):\n",
    "                edge_weight[would_be_change_edge_list] += 1\n",
    "                # print('if active:',edge_weight[would_be_change_edge_list])\n",
    "                pop = popular.loc[user_id[vertx]].tolist()\n",
    "                # print('popular: ',(float(pop[1])+1))\n",
    "                for num in would_be_change_edge_list:\n",
    "                    edge_weight[num] = edge_weight[num] * (float(pop[1]) / 5)   # /5 是要壓低pop的數值\n",
    "                # print('after popular: ',edge_weight[would_be_change_edge_list])\n",
    "                # edge_weight[would_be_change_edge_list] = np.round(edge_weight[would_be_change_edge_list])\n",
    "                # print('final: ',edge_weight[would_be_change_edge_list])\n",
    "        except:\n",
    "            isolate_count += 1\n",
    "    # edge_weight = np_norm(edge_weight)\n",
    "    # graph_weight = th.from_numpy(edge_weight).to(th.float32)\n",
    "    # base_graph.edata['weight'] = graph_weight\n",
    "    print('edge weight: ', len(edge_weight))\n",
    "    base_graph.edata['weight'] = th.from_numpy(edge_weight).to(th.float32)\n",
    "    # print(' graph edata: ', base_graph.edata['weight'])\n",
    "    base_graph.ndata['self_weight'] = th.from_numpy(np.full((base_graph.number_of_nodes(),1),np.median(edge_weight,axis=0))).to(th.float32)\n",
    "\n",
    "    for times in range(5):#5\n",
    "        mask = th.rand(base_graph.number_of_nodes())\n",
    "        train_mask = mask < 0.8\n",
    "        train_mask = train_mask.numpy()\n",
    "        test_mask = mask >= 0.8\n",
    "        test_mask = test_mask.numpy()\n",
    "        if not os.path.exists(Save_train_Path):\n",
    "            os.makedirs(Save_train_Path)\n",
    "        if not os.path.exists(Save_test_Path):\n",
    "            os.makedirs(Save_test_Path)\n",
    "        #儲存train, test的mask\n",
    "        np.save(Save_train_Path + str(times), train_mask)\n",
    "        np.save(Save_test_Path + str(times), test_mask)\n",
    "    # print(base_graph)\n",
    "    #將graph的結果輸出\n",
    "    save_graphs(all_base_graph_dir[per_tweet] + str(tweet_number) + \".bin\", [base_graph])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11934\n"
     ]
    }
   ],
   "source": [
    "#建立無weight有向圖\n",
    "\n",
    "#把user id給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')                                       \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "vertx_number = len(user_id)\n",
    "del user_id\n",
    "\n",
    "src_node = np.load(elon_mask_base_graph_data_dir + 'Graph/source_vertx_collection.npy')\n",
    "dst_node = np.load(elon_mask_base_graph_data_dir + 'Graph/destion_vertx_collection.npy')\n",
    "model_graph_dir = elon_mask_base_graph_data_dir + 'user_feature.npy'\n",
    "vertx_feature = np.load(model_graph_dir)\n",
    "#將前面得到的src和dst的矩陣當作資料，一個個加入到圖中\n",
    "base_graph = dgl.graph(([],[]),num_nodes = vertx_number)                                                  \n",
    "base_graph.add_edges(src_node.astype(int),dst_node.astype(int))\n",
    "graph_feature = th.from_numpy(vertx_feature).to(th.float32)\n",
    "base_graph.ndata['feature'] = graph_feature\n",
    "# print(base_graph.edata['weight'])\n",
    "#將graph的結果輸出\n",
    "print(base_graph.number_of_nodes())\n",
    "# save_graphs(elon_mask_base_graph_data_dir + \"Graph/GAT_base_graph.bin\", [base_graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2/11~15 no weighted module graph\n",
    "all_base_graph_dir = list()\n",
    "#把user id給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')           \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "\n",
    "#需要建立幾個model\n",
    "for it in os.scandir(elon_mask_base_graph_data_dir + 'base_graph_for_model/'):    \n",
    "    if it.is_dir():\n",
    "        all_base_graph_dir.append(it.path + \"/\")\n",
    "# print(\"all_base_graph_dirdir: \", all_base_graph_dir)\n",
    "\n",
    "popular = pd.read_excel(user_profile_dir + 'Profile_new.xlsx')\n",
    "popular[['id','popular']] = popular[['id','popular']].astype(str)\n",
    "popular = popular[['id','popular']]\n",
    "# print(popular)\n",
    "\n",
    "#將建好的graph讀取出來之後把label放上去 並且對應edge去檢查是否active，再去更改edge_weight\n",
    "dataset,labels =  load_graphs(elon_mask_base_graph_data_dir + \"Graph/GAT_base_graph.bin\")     \n",
    "base_graph = dataset[0]\n",
    "u,v = base_graph.edges()\n",
    "src_vertx = np.array(u)\n",
    "# edge_no_weight = base_graph.edata['weight'].numpy()\n",
    "# print(sum(edge_no_weight))\n",
    "\n",
    "for per_tweet in range(len(all_base_graph_dir)):#\n",
    "    Save_train_Path = all_base_graph_dir[per_tweet] + 'train/'\n",
    "    Save_test_Path = all_base_graph_dir[per_tweet] + 'test/'\n",
    "    tweet_number = all_base_graph_dir[per_tweet].split(\"/\")\n",
    "    tweet_number = tweet_number[-2]\n",
    "\n",
    "    label = np.load(elon_mask_base_graph_data_dir + 'label/label_for_' + str(tweet_number) + '.npy')\n",
    "    base_graph.ndata['label'] = th.from_numpy(label).type(th.LongTensor)\n",
    "    isolate_count = 0\n",
    "    # for vertx in user_id.keys():\n",
    "    #     try:\n",
    "    #         would_be_change_edge_list = np.where(src_vertx == user_id[vertx])\n",
    "    #         # print('label_vertx: ', label[user_id[vertx]])\n",
    "    #         # print('orgin:',edge_weight[would_be_change_edge_list])\n",
    "    #         if(label[user_id[vertx]] == 1):\n",
    "    #             edge_no_weight[would_be_change_edge_list] += 1\n",
    "    #             # print('if active:',edge_no_weight[would_be_change_edge_list])\n",
    "    #             pop = popular.loc[user_id[vertx]].tolist()\n",
    "    #             # print('popular: ',(float(pop[1])+1))\n",
    "    #             for num in would_be_change_edge_list:\n",
    "    #                 print(edge_no_weight[num])\n",
    "    #                 edge_no_weight[num] = edge_no_weight[num] * (float(pop[1]) / 5)   # /5 是要壓低pop的數值\n",
    "    #             # print('after popular: ',edge_no_weight[would_be_change_edge_list])\n",
    "    #             edge_no_weight[would_be_change_edge_list] = np.round(edge_weight[would_be_change_edge_list])\n",
    "    #             # print('final: ',edge_no_weight[would_be_change_edge_list])\n",
    "    #     except:\n",
    "    #         isolate_count += 1   \n",
    "    # edge_weight = np_norm(edge_weight)\n",
    "    # graph_weight = th.from_numpy(edge_weight).to(th.float32)\n",
    "    # base_graph.edata['weight'] = graph_weight\n",
    "    # print('edge_no_weight：', sum(edge_no_weight))\n",
    "\n",
    "    # base_graph.edata['weight'] = th.from_numpy(edge_weight).to(th.int32)\n",
    "    # print('graph node count: ', base_graph.number_of_nodes())\n",
    "    # base_graph.ndata['self_weight'] = th.from_numpy(np.full((base_graph.number_of_nodes(), 1))).to(th.float32)\n",
    "    # print(\"base_graph.ndata['self_weight']: \", base_graph.ndata['self_weight'])\n",
    "\n",
    "    for times in range(5):#5\n",
    "        mask = th.rand(base_graph.number_of_nodes())\n",
    "        train_mask = mask < 0.8\n",
    "        train_mask = train_mask.numpy()\n",
    "        test_mask = mask >= 0.8\n",
    "        test_mask = test_mask.numpy()\n",
    "        if not os.path.exists(Save_train_Path):\n",
    "            os.makedirs(Save_train_Path)\n",
    "        if not os.path.exists(Save_test_Path):\n",
    "            os.makedirs(Save_test_Path)\n",
    "        #儲存train, test的mask\n",
    "        np.save(Save_train_Path + str(times), train_mask)\n",
    "        np.save(Save_test_Path + str(times), test_mask)\n",
    "    #將graph的結果輸出\n",
    "    save_graphs(all_base_graph_dir[per_tweet] + str(tweet_number) + \"noweight.bin\", [base_graph])\n",
    "print(base_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先跑elon_model_graph_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN整理訓練結果，紀錄accuracy、precision、recall、F1\n",
    "\n",
    "all_model_json_file_dir = list()\n",
    "all_result_arrange = list()\n",
    "#需要蒐集幾個model結果\n",
    "for it in os.scandir(base_graph_dir):    \n",
    "    if it.is_dir():\n",
    "        all_model_json_file_dir.append(it.path + \"/\")\n",
    "\n",
    "print(all_model_json_file_dir[2])\n",
    "for every_graph in range(27):#27\n",
    "    result_arrange = {}\n",
    "    for every_batch in range(5):#5\n",
    "        # print(every_graph)\n",
    "        # print(every_batch)\n",
    "        #把GCN的user id給讀入\n",
    "        result = open(all_model_json_file_dir[every_graph] + str(every_batch) + '_round_result_edge_weight_Adjustment.json')           \n",
    "        ever_graph_result = json.load(result)\n",
    "        result.close()\n",
    "        result_arrange[every_batch] = ever_graph_result\n",
    "        # print(result_arrange)\n",
    "    all_result_arrange.append(pd.DataFrame.from_dict(result_arrange))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_base_graph_data_dir + 'model_result_modify_collection.xlsx') as writer:\n",
    "    for model_result in range(len(all_result_arrange)):\n",
    "        all_result_arrange[model_result].to_excel(writer, sheet_name = 'model_for_' + str(model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAT(No edge weight)整理訓練結果，紀錄accuracy、precision、recall、F1\n",
    "\n",
    "all_model_json_file_dir = list()\n",
    "GAT_all_result_arrange = list()\n",
    "#需要蒐集幾個model結果\n",
    "for it in os.scandir(gat_no_edge_weight_base_graph_dir):    \n",
    "    if it.is_dir():\n",
    "        all_model_json_file_dir.append(it.path + \"/\")\n",
    "print(all_model_json_file_dir)\n",
    "# for every_graph in range(27):\n",
    "#     GAT_result_arrange = {}\n",
    "#     for every_batch in range(5):#5\n",
    "#         #把GAT的user id給讀入\n",
    "#         GAT_result = open(all_model_json_file_dir[every_graph] + str(every_batch) + '_round_result_No_edge_weight_Adjustment_GAT.json')\n",
    "#         GAT_ever_graph_result = json.load(GAT_result)\n",
    "#         GAT_result.close()\n",
    "#         GAT_result_arrange[every_batch] = GAT_ever_graph_result\n",
    "#     GAT_all_result_arrange.append(pd.DataFrame.from_dict(GAT_result_arrange))\n",
    "            \n",
    "# with pd.ExcelWriter(elon_mask_base_graph_data_dir + '_model_result_modify_collection_GAT_no_weight0725.xlsx') as writer:\n",
    "#     for model_result in range(len(GAT_all_result_arrange)):\n",
    "#         GAT_all_result_arrange[model_result].to_excel(writer, sheet_name = 'model_for_' + str(model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將2/16 ~ 2/20 retweet時間表建立並且儲存\n",
    "all_base_graph_dir = list()\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')                                        #把userid給讀入\n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "\n",
    "target_tweet = pd.read_excel(elon_mask_base_graph_data_dir + 'user_tweets/RetweetTime_Distribution.xlsx', sheet_name='retweet time')    #這邊是把轉推時間分布讀進來\n",
    "target_tweet = target_tweet.astype(str)\n",
    "#這邊把預測推文的轉推數讀進來\n",
    "target_tweet_list = pd.read_excel(elon_mask_base_graph_data_dir + 'user_tweets/RetweetTime_Distribution.xlsx', sheet_name='total count of each hr')    \n",
    "#本次預測推文的原轉推人數超過 \" 1000人 \"\n",
    "raw_target_tweet_list = target_tweet_list['tweet id'][list(np.where(np.array(target_tweet_list['48hr']) > 1000)[0])].values.tolist()                        \n",
    "remain_target_user = target_tweet[target_tweet['id'].isin(user_id.keys())].copy()\n",
    "\n",
    "all_data_graph_label = np.zeros((len(raw_target_tweet_list),len(user_id),12))\n",
    "count = 0\n",
    "a = 0\n",
    "for target_tweet_id in raw_target_tweet_list:\n",
    "    target_tweet_retweet_time_mask = remain_target_user['referenced'].isin([str(target_tweet_id)])\n",
    "    target_tweet_retweet_time_list = list(remain_target_user['time_delta'][target_tweet_retweet_time_mask])\n",
    "    target_tweet_retweet_user = list(remain_target_user['id'][target_tweet_retweet_time_mask])\n",
    "    for number in range(len(target_tweet_retweet_user)):\n",
    "        if(all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] != 0):\n",
    "            if(int(target_tweet_retweet_time_list[number]) < all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1]):\n",
    "                if(int(target_tweet_retweet_time_list[number]) <= 1):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][0:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 1\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 2):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][1:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 2\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 4):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][2:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 4\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 6):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][3:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 6\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 8):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][4:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 8\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 10):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][5:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 10\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 12):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][6:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 12\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 16):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][7:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 16\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 20):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][8:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 20\n",
    "                elif(int(target_tweet_retweet_time_list[number]) <= 24):\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][9:11] = 1\n",
    "                    all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 24\n",
    "                    \n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 1):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][0:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 1\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 2):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][1:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 2\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 4):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][2:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 4\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 6):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][3:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 6\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 8):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][4:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 8\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 10):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][5:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 10\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 12):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][6:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 12\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 16):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][7:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 16\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 20):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][8:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 20\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 24):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][9:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 24\n",
    "        elif(int(target_tweet_retweet_time_list[number]) <= 48):\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][10:11] = 1\n",
    "            all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]][-1] = 48\n",
    "        # print(all_data_graph_label[count][user_id[str(target_tweet_retweet_user[number])]])\n",
    "    # break\n",
    "    count += 1\n",
    "# print(a)\n",
    "\n",
    "#建資料夾\n",
    "if not os.path.isdir(elon_mask_base_graph_data_dir + 'retweet_distribution/'):\n",
    "    os.mkdir(elon_mask_base_graph_data_dir + 'retweet_distribution/')\n",
    "\n",
    "for graph_number in range(all_data_graph_label.shape[0]):\n",
    "    np.save(elon_mask_base_graph_data_dir + 'retweet_distribution/' + str(graph_number) +\n",
    "            '_graph_retweet_distribution_rewrite.npy', all_data_graph_label[graph_number])\n",
    "    #拿48hr的人作為retweet_count label\n",
    "    np.save(elon_mask_test_graph_data_dir + 'label/label_for_' + str(graph_number), all_data_graph_label[graph_number][:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立2/15 ~ 2/19的新版前29篇的['1h', '2h', '4h', '6h', '8h', '10h', '12h', '16h', '20h', '24h', '48h']retweet count\n",
    "\n",
    "doc_num = len(os.listdir('D:/GCN_Twitter/ElonMusk/2023-02-20/retweet_distribution'))\n",
    "total_retweet_list  = list()\n",
    "for i in range(doc_num):\n",
    "    retweet_list = list()\n",
    "    npFile = np.load(base_tweet_retweet_distribution + str(i) + '_graph_retweet_distribution_rewrite.npy')\n",
    "    for timestamp in range(11):\n",
    "        retweet_count = sum(npFile[:, timestamp])\n",
    "        retweet_list.append(retweet_count)\n",
    "    total_retweet_list.append(retweet_list)\n",
    "# print(\"Sum: \", total_retweet_list)\n",
    "print(len(total_retweet_list))\n",
    "df = pd.DataFrame(total_retweet_list, \n",
    "                  columns = ['1h', '2h', '4h', '6h', '8h', '10h', '12h', '16h', '20h', '24h', '48h'])\n",
    "\n",
    "# print(df)\n",
    "with pd.ExcelWriter(elon_mask_base_graph_data_dir + 'user_tweets/newRetweetTime_Distribution.xlsx') as writer:\n",
    "    df.to_excel(writer)\n",
    "    # for retweet_count_result in range(len(total_retweet_list)):\n",
    "    #     for retweet_timestamp in range(len(retweet_list)):\n",
    "    #         print(type(df[retweet_timestamp][retweet_count_result]))\n",
    "    #         df[retweet_timestamp][retweet_count_result].to_excel(writer, sheet_name = 'total count of each hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base remove\n",
    "#將可能超過目標數的tweet轉推紀錄的檔名紀錄\n",
    "# np.load('D:\\\\GCN_Twitter\\\\ElonMusk\\\\2023-02-16\\\\retweet_distribution\\\\0_graph_retweet_distribution_rewrite.npy')\n",
    "raw_retweet_record_file = []                                                                                \n",
    "for path in os.listdir(base_tweet_retweet_distribution):\n",
    "    if os.path.isfile(os.path.join(base_tweet_retweet_distribution, path)):\n",
    "            raw_retweet_record_file.append(path)\n",
    "\n",
    "for nubmer in range(len(raw_retweet_record_file)):                                                          \n",
    "    feature = np.load(base_tweet_retweet_distribution + str(nubmer) + '_graph_retweet_distribution_rewrite.npy')\n",
    "    # print('號碼 {} 的graph 有 {} 的轉推數'.format(nubmer, len(np.where(feature[:,9] == 1)[0])))\n",
    "    #計算有哪些tweet有超過標準，這邊標準為\" 1200人 \"\n",
    "    if len(np.where(feature[:,9] == 1)[0]) < 1200 :\n",
    "        print('號碼 {} 的graph 有 {} 的轉推數'.format(nubmer, len(np.where(feature[:,9] == 1)[0])))\n",
    "        os.remove(base_tweet_retweet_distribution + str(nubmer) + '_graph_retweet_distribution_rewrite.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "doc_num = len(os.listdir('D:/GCN_Twitter/ElonMusk/2023-02-20/label/'))\n",
    "for i in range(doc_num):\n",
    "    count = 0\n",
    "    label_l = np.load(elon_mask_test_graph_data_dir + 'label/label_for_' + str(i) + '.npy')\n",
    "    retweet_count = sum(label_l)\n",
    "    print('retweet_count: ', sum(label_l))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊將2/16~20的data graph 給建出來\n",
    "\n",
    "#以轉推時間的長短做為加權依據。從轉推時間到現在作為半衰期的指數，最終趨近於0.5\n",
    "def weight_Half_life(retweet_time,now_time):\n",
    "    return  0.5 + (1 - 0.5) * (1 / (2**(now_time - retweet_time)))             \n",
    "\n",
    "all_data_graph_dir = list()\n",
    "#把userid給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')           \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "\n",
    "label_list = list()\n",
    "for it in os.scandir(test_tweet_retweet_distribution):    #需要建立幾個model\n",
    "    if it.is_file():\n",
    "        label_list.append(it.path + \"/\")\n",
    "     \n",
    "#創立等等要放data graph的資料夾        \n",
    "for tweet_number in range(len(label_list)):     \n",
    "    if not os.path.isdir(elon_mask_test_graph_data_dir + 'data_graph/' + str(tweet_number)):\n",
    "        os.mkdir(elon_mask_test_graph_data_dir + 'data_graph/' + str(tweet_number))\n",
    "    for time_point in range(10):\n",
    "        if not os.path.isdir(elon_mask_test_graph_data_dir + 'data_graph/' + str(tweet_number) + '/' + str(time_point)):\n",
    "            os.mkdir(elon_mask_test_graph_data_dir + 'data_graph/' + str(tweet_number) + '/' + str(time_point))\n",
    "popular = pd.read_excel(user_profile_dir + 'Profile_new.xlsx')\n",
    "popular[['id','popular']] = popular[['id','popular']].astype(str)\n",
    "popular = popular[['id','popular']]\n",
    "#將建好的graph讀取出來之後把label放上去 並且對應edge去檢查是否active，再去更改edge_weight\n",
    "dataset, labels =  load_graphs(elon_mask_base_graph_data_dir + \"Graph/base_graph.bin\")     \n",
    "base_graph = dataset[0]\n",
    "u, v = base_graph.edges()\n",
    "src_vertx = np.array(u)\n",
    "edge_weight = base_graph.edata['weight'].numpy()\n",
    "retweet_time_point_list = [1, 2, 4, 6, 8, 10, 12, 16, 20, 24]\n",
    "print('before_edge_weight: ', edge_weight)\n",
    "for per_tweet in range(len(label_list)):#\n",
    "    tweet_numbers = label_list[per_tweet].split(\"_\")\n",
    "    tweet_numbers = tweet_numbers[2].split(\"/\")[1]\n",
    "    # print( tweet_numbers)\n",
    "    label = np.load(test_tweet_label + 'label_for_' + str(tweet_numbers) + '.npy')\n",
    "    retweet_distribution = np.load(base_tweet_retweet_distribution+str(tweet_numbers) + '_graph_retweet_distribution_rewrite.npy')\n",
    "\n",
    "    for time_points in range(10):             \n",
    "        now_active = retweet_distribution[:,time_points]\n",
    "        active_retweet_time = retweet_distribution[:,10]\n",
    "        base_graph.ndata['label'] = th.from_numpy(label).type(th.LongTensor)\n",
    "        isolate_count = 0\n",
    "        for vertx in user_id.keys():\n",
    "            try:\n",
    "                would_be_change_edge_list = np.where(src_vertx == user_id[vertx])   #首先檢查src是否在target user裡\n",
    "                # print('orgin:',edge_weight[would_be_change_edge_list])\n",
    "                if(now_active[user_id[vertx]] == 1):                                  #在檢查是否為active\n",
    "                    edge_weight[would_be_change_edge_list] += 1\n",
    "                    # print('node number: ',user_id[vertx])\n",
    "                    # print('推文轉推時的時間',active_retweet_time[user_id[vertx]])\n",
    "                    # print('現在的時間',retweet_time_point_list[time_points])\n",
    "                    # print('半衰期的結果',weight_Half_life(active_retweet_time[user_id[vertx]],retweet_time_point_list[time_points]))\n",
    "                    # print('if active:',edge_weight[would_be_change_edge_list])\n",
    "                    pop = popular.loc[user_id[vertx]].tolist()                      #是active時要做 weight * pop\n",
    "                    # print('popular: ',(float(pop[1])+1))\n",
    "                    for num in would_be_change_edge_list:                           #將該user所有的out degree做 weight *pop\n",
    "                        edge_weight[num] = edge_weight[num] * (float(pop[1]) / 5)   # /5 是要壓低pop的數值\n",
    "                    # print('after popular: ',edge_weight[would_be_change_edge_list])\n",
    "                    # edge_weight[would_be_change_edge_list] = np.round(edge_weight[would_be_change_edge_list]) #將結果寫回原本的weight上\n",
    "                    # print('final: ',edge_weight[would_be_change_edge_list])\n",
    "            except:\n",
    "                isolate_count += 1\n",
    "        print('after_edge_weight: ', edge_weight)        \n",
    "        base_graph.edata['weight'] = th.from_numpy(edge_weight).to(th.float32)\n",
    "        # print(base_graph.edata['weight'])\n",
    "        #每個vertx在gcn時要一起考慮自身的feature ，所以給它一個weight 讓其考慮自身時可以跟周邊的edge weight類似，這邊使用中位數\n",
    "        base_graph.ndata['self_weight'] = th.from_numpy(np.full((base_graph.number_of_nodes(),1),np.median(edge_weight,axis=0))).to(th.float32) \n",
    "        save_graphs(elon_mask_test_graph_data_dir + 'data_graph/' + str(tweet_numbers) + '/' +\n",
    "                    str(time_points) + '/' + str(tweet_numbers) + \"_rewrite.bin\", [base_graph]) #將graph的結果輸出\n",
    "    # print(base_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊將2/16~20的data graph(No edge weight) 給建出來\n",
    "\n",
    "#以轉推時間的長短做為加權依據。從轉推時間到現在作為半衰期的指數，最終趨近於0.5\n",
    "def weight_Half_life(retweet_time,now_time):\n",
    "    return  0.5 + (1 - 0.5) * (1 / (2**(now_time - retweet_time)))             \n",
    "\n",
    "all_data_graph_dir = list()\n",
    "#把userid給讀入\n",
    "id = open(elon_mask_base_graph_data_dir + 'Graph/encoding_table.json')           \n",
    "user_id = json.load(id)\n",
    "id.close()\n",
    "\n",
    "label_list = list()\n",
    "for it in os.scandir(test_tweet_retweet_distribution):    #需要建立幾個model\n",
    "    if it.is_file():\n",
    "        label_list.append(it.path + \"/\")\n",
    "     \n",
    "#創立等等要放data graph的資料夾        \n",
    "for tweet_number in range(len(label_list)):     \n",
    "    if not os.path.isdir(elon_mask_test_graph_data_dir + 'no_weight_data_graph/' + str(tweet_number)):\n",
    "        os.mkdir(elon_mask_test_graph_data_dir + 'no_weight_data_graph/' + str(tweet_number))\n",
    "    for time_point in range(10):\n",
    "        if not os.path.isdir(elon_mask_test_graph_data_dir + 'no_weight_data_graph/' + str(tweet_number) + '/' + str(time_point)):\n",
    "            os.mkdir(elon_mask_test_graph_data_dir + 'no_weight_data_graph/' + str(tweet_number) + '/' + str(time_point))\n",
    "\n",
    "popular = pd.read_excel(user_profile_dir + 'Profile_new.xlsx')\n",
    "popular[['id','popular']] = popular[['id','popular']].astype(str)\n",
    "popular = popular[['id','popular']]\n",
    "#將建好的graph讀取出來之後把label放上去\n",
    "dataset, labels =  load_graphs(elon_mask_base_graph_data_dir + \"Graph/GAT_base_graph.bin\")     \n",
    "base_graph = dataset[0]\n",
    "u, v = base_graph.edges()\n",
    "src_vertx = np.array(u)\n",
    "retweet_time_point_list = [1, 2, 4, 6, 8, 10, 12, 16, 20, 24]\n",
    "\n",
    "for per_tweet in range(len(label_list)):#\n",
    "    tweet_numbers = label_list[per_tweet].split(\"_\")\n",
    "    tweet_numbers = tweet_numbers[2].split(\"/\")[1]\n",
    "    # print( tweet_numbers)\n",
    "    label = np.load(test_tweet_label + 'label_for_' + str(tweet_numbers) + '.npy')\n",
    "    retweet_distribution = np.load(base_tweet_retweet_distribution + str(tweet_numbers) + '_graph_retweet_distribution_rewrite.npy')\n",
    "\n",
    "    for time_points in range(10):             \n",
    "        now_active = retweet_distribution[:,time_points]\n",
    "        active_retweet_time = retweet_distribution[:,10]\n",
    "        base_graph.ndata['label'] = th.from_numpy(label).type(th.LongTensor)\n",
    "\n",
    "        #將graph的結果輸出\n",
    "        save_graphs(elon_mask_test_graph_data_dir + 'no_weight_data_graph/' + str(tweet_numbers) + '/' +\n",
    "                    str(time_points) + '/' + str(tweet_numbers) + \"_rewrite.bin\", [base_graph]) \n",
    "    print(base_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑 elon_time_graph_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2/16~20的預測結果蒐集\n",
    "all_predict_json_file_dir = list()\n",
    "for it in os.scandir(predict_graph_dir):    #需要蒐集幾個model結果\n",
    "    if it.is_dir():\n",
    "        all_predict_json_file_dir.append(it.path + \"/\")\n",
    "# print(all_predict_json_file_dir)\n",
    "all_tag_result_arrange = list()\n",
    "all_high_retweet_count_result_arrange = list()\n",
    "all_middle2_retweet_count_result_arrange = list()\n",
    "all_middle_retweet_count_result_arrange = list()\n",
    "all_low_retweet_count_result_arrange = list()\n",
    "all_first_retweet_distribution_result_arrange = list()\n",
    "all_second_retweet_distribution_result_arrange = list()\n",
    "all_third_retweet_distribution_result_arrange = list()\n",
    "all_forth_retweet_distribution_result_arrange = list()\n",
    "all_fifth_retweet_distribution_result_arrange = list()\n",
    "all_sixth_retweet_distribution_result_arrange = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for every_graph in all_predict_json_file_dir:#29\n",
    "    tag_result_template = {}\n",
    "    retweet_count_high_result_template = {}\n",
    "    retweet_count_middle2_result_template = {}\n",
    "    retweet_count_middle_result_template = {}\n",
    "    retweet_count_low_result_template = {}\n",
    "    first_retweet_distribution_result_template = {}\n",
    "    second_retweet_distribution_result_template = {}\n",
    "    third_retweet_distribution_result_template = {}\n",
    "    forth_retweet_distribution_result_template = {}    \n",
    "    fifth_retweet_distribution_result_template = {}\n",
    "    sixth_retweet_distribution_result_template = {}\n",
    "\n",
    "    graph_number = re.split('/', every_graph)[-2]\n",
    "    # print('graph number: ', graph_number)\n",
    "    for every_time_batch in range(10):#10\n",
    "        for it in os.scandir(every_graph + str(every_time_batch)):    #需要蒐集幾個test結果\n",
    "            if it.is_file():\n",
    "                file_name = re.split('/|\\\\\\\\', it.path)[-1]\n",
    "                # print(file_name)\n",
    "                file_name_part = re.split('/|\\\\\\\\|_', it.path)[10:]\n",
    "                # print(type(file_name_part[-1][-12:]))\n",
    "                # 為啥要[-12:]\n",
    "                if (file_name_part[-1][-12:] == 'result.json' ):\n",
    "                    # print(file_name_part[1])\n",
    "                    if(file_name_part[0] == 'tag'):\n",
    "                        #把tag_result給讀入\n",
    "                        tag_result = open(every_graph + str(every_time_batch) + '/' + file_name)     \n",
    "                        ever_graph_result = json.load(tag_result)\n",
    "                        tag_result.close()\n",
    "                        # print(ever_graph_result)\n",
    "                        tag_result_template[str(graph_number) + '_' + str(every_time_batch) + '_round'] = ever_graph_result\n",
    "\n",
    "                    elif(file_name_part[0] == 'high'):\n",
    "                        retweet_count_result = open(every_graph + str(every_time_batch) + '/' + file_name)           #把retweet_count_result給讀入\n",
    "                        ever_graph_result = json.load(retweet_count_result)\n",
    "                        retweet_count_result.close()\n",
    "                        retweet_count_high_result_template[str(graph_number) + '_' + str(every_time_batch)] = ever_graph_result\n",
    "                    elif(file_name_part[0] == 'middle2'):\n",
    "                        retweet_count_result = open(every_graph + str(every_time_batch) + '/' + file_name)           #把retweet_count_result給讀入\n",
    "                        ever_graph_result = json.load(retweet_count_result)\n",
    "                        retweet_count_result.close()\n",
    "                        retweet_count_middle2_result_template[str(graph_number) + '_' + str(every_time_batch)] = ever_graph_result\n",
    "                    elif(file_name_part[0] == 'middle'):\n",
    "                        retweet_count_result = open(every_graph + str(every_time_batch) + '/' + file_name)           #把retweet_count_result給讀入\n",
    "                        ever_graph_result = json.load(retweet_count_result)\n",
    "                        retweet_count_result.close()\n",
    "                        retweet_count_middle_result_template[str(graph_number) + '_' + str(every_time_batch)] = ever_graph_result\n",
    "                    elif(file_name_part[0] == 'low'):\n",
    "                        retweet_count_result = open(every_graph + str(every_time_batch) + '/' + file_name)           #把retweet_count_result給讀入\n",
    "                        ever_graph_result = json.load(retweet_count_result)\n",
    "                        retweet_count_result.close()\n",
    "                        retweet_count_low_result_template[str(graph_number) + '_' + str(every_time_batch)] = ever_graph_result\n",
    "                    else:\n",
    "                        retweet_distribution_result = open(every_graph + str(every_time_batch) + '/' + file_name)           #把retweet_distribution_result給讀入\n",
    "                        ever_graph_result = json.load(retweet_distribution_result)\n",
    "                        retweet_distribution_result.close()\n",
    "                        #找in為開頭的檔名\n",
    "                        if(file_name_part[0] == 'in'):\n",
    "                            first_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result\n",
    "                        #找between', '25'為開頭的檔名    \n",
    "                        elif(file_name_part[1] == '25'):\n",
    "                            second_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result\n",
    "                        #找between', '31'為開頭的檔名      \n",
    "                        elif(file_name_part[1] == '31'):\n",
    "                            third_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result\n",
    "                        #找between', '50'為開頭的檔名      \n",
    "                        elif(file_name_part[1] == '50'):\n",
    "                            forth_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result\n",
    "                        #找between', '60'為開頭的檔名     \n",
    "                        elif(file_name_part[1] == '60'):\n",
    "                            fifth_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result \n",
    "                        else:\n",
    "                            sixth_retweet_distribution_result_template[str(graph_number)+'_'+str(every_time_batch)] = ever_graph_result\n",
    "    all_tag_result_arrange.append(pd.DataFrame.from_dict(tag_result_template))\n",
    "    all_high_retweet_count_result_arrange.append(pd.DataFrame.from_dict(retweet_count_high_result_template))\n",
    "    all_middle2_retweet_count_result_arrange.append(pd.DataFrame.from_dict(retweet_count_middle2_result_template))\n",
    "    all_middle_retweet_count_result_arrange.append(pd.DataFrame.from_dict(retweet_count_middle_result_template))\n",
    "    all_low_retweet_count_result_arrange.append(pd.DataFrame.from_dict(retweet_count_low_result_template))\n",
    "    if bool(first_retweet_distribution_result_template):\n",
    "        all_first_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(first_retweet_distribution_result_template))\n",
    "        all_second_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(second_retweet_distribution_result_template))\n",
    "        all_third_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(third_retweet_distribution_result_template))\n",
    "        all_forth_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(forth_retweet_distribution_result_template))\n",
    "        all_fifth_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(fifth_retweet_distribution_result_template))\n",
    "        all_sixth_retweet_distribution_result_arrange.append(pd.DataFrame.from_dict(sixth_retweet_distribution_result_template))\n",
    "    # if (graph_number == '15'):\n",
    "    #     break\n",
    "    # print('tag result: \\n',all_tag_result_arrange[0])\n",
    "    # print('retweet count result: \\n',all_retweet_count_result_arrange)\n",
    "    # print('retweet distribution result: \\n',all_retweet_distribution_result_arrange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/tags_result_rewrite(test).xlsx') as writer:\n",
    "#     for tag_result_number in range(len(all_tag_result_arrange)):\n",
    "#         tag_result_num = all_tag_result_arrange[tag_result_number]\n",
    "#         # print(tag_result_num.columns.values)\n",
    "#         graph_num = re.split('_', tag_result_num.columns.values[0])\n",
    "#         all_tag_result_arrange[tag_result_number].to_excel(writer, sheet_name = 'tag_for_' + str(graph_num[0]))\n",
    "\n",
    "# with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/high_retweet_count_result_rewrite.xlsx') as writer:\n",
    "#     for retweet_count_result_number in range(len(all_high_retweet_count_result_arrange)):\n",
    "#         graph_num = re.split('_',all_high_retweet_count_result_arrange[retweet_count_result_number].columns.values[0])[0]\n",
    "#         all_high_retweet_count_result_arrange[retweet_count_result_number].to_excel(writer, sheet_name = 'count_for_' + str(graph_num))\n",
    "\n",
    "# with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/middle2_retweet_count_result_rewrite.xlsx') as writer:\n",
    "#     for retweet_count_result_number in range(len(all_middle2_retweet_count_result_arrange)):\n",
    "#         graph_num = re.split('_',all_middle2_retweet_count_result_arrange[retweet_count_result_number].columns.values[0])[0]\n",
    "#         all_middle2_retweet_count_result_arrange[retweet_count_result_number].to_excel(writer, sheet_name = 'count_for_' + str(graph_num))\n",
    "\n",
    "# with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/middle_retweet_count_result_rewrite.xlsx') as writer:\n",
    "#     for retweet_count_result_number in range(len(all_middle_retweet_count_result_arrange)):\n",
    "#         # print(all_middle_retweet_count_result_arrange[retweet_count_result_number])\n",
    "#         graph_num = re.split('_',all_middle_retweet_count_result_arrange[retweet_count_result_number].columns.values[0])[0]\n",
    "#         all_middle_retweet_count_result_arrange[retweet_count_result_number].to_excel(writer, sheet_name='count_for_'+str(graph_num))\n",
    "\n",
    "# with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/low_retweet_count_result_rewrite.xlsx') as writer:\n",
    "#     for retweet_count_result_number in range(len(all_low_retweet_count_result_arrange)):\n",
    "#         graph_num = re.split('_',all_low_retweet_count_result_arrange[retweet_count_result_number].columns.values[0])[0]\n",
    "#         all_low_retweet_count_result_arrange[retweet_count_result_number].to_excel(writer, sheet_name='count_for_'+str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/first_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_first_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_first_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_first_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_' + str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/second_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_second_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_second_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_second_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_'+str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/third_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_third_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_third_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_third_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_'+str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/forth_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_forth_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_forth_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_forth_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_'+str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/fifth_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_fifth_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_fifth_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_fifth_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_'+str(graph_num))\n",
    "\n",
    "with pd.ExcelWriter(elon_mask_test_graph_data_dir + 'prediction_result0721/sixth_retweet_distribution_result_rewrite.xlsx') as writer:\n",
    "    for retweet_distribution_number in range(len(all_sixth_retweet_distribution_result_arrange)):\n",
    "        graph_num = re.split('_',all_sixth_retweet_distribution_result_arrange[retweet_distribution_number].columns.values[0])[0]\n",
    "        all_sixth_retweet_distribution_result_arrange[retweet_distribution_number].to_excel(writer, sheet_name='distribution_for_'+str(graph_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
